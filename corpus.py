# -*- coding: utf-8 -*-
"""TBCdI Corpus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TbUVJ82ltcSXZh3IhEIyPruClH9M4OXc
"""

# Gabriel Marchioro Klein
#
# Sua  tarefa  será  transformar  um  conjunto  de  5  sites,  sobre  o  tema  de  processamento  de 
# linguagem natural em um conjunto de cinco listas distintas de sentenças. Ou seja, você fará uma função 
# que, usando a biblioteca Beautifull Soap, faça a requisição de uma url, e extrai todas as sentenças desta 
# url. Duas condições são importantes:  
# a) A página web (url) deve apontar para uma página web em inglês contendo, não menos que 
# 1000 palavras.  
# b) O texto desta página deverá ser transformado em um array de senteças.  
# 
# Para separar as sentenças você pode usar os sinais de pontuação ou as funções da biblibioteca 
# Spacy.

from bs4 import BeautifulSoup
import requests
import spacy

spacyEnglish = spacy.load("en_core_web_sm")
spacyEnglish.add_pipe('sentencizer')

links = [
  "https://en.wikipedia.org/wiki/Natural_language_processing", 
  "https://www.tableau.com/learn/articles/natural-language-processing-examples",
  "https://www.geeksforgeeks.org/natural-language-processing-overview/",
  "https://monkeylearn.com/natural-language-processing/",
  "https://cloud.google.com/learn/what-is-natural-language-processing"
]

for link in links:
  page = requests.get(link)
  soup = BeautifulSoup(page.content, 'html.parser')
  corpus = []
  for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "li"]):
    text = tag.get_text()
    strippedText = text.strip("\n")
    strippedText = text.strip()
    if len(strippedText) > 0:
      spacyText = spacyEnglish(strippedText)
      for sentence in spacyText.sents:
        corpus.append(sentence.text)
  print(f'Corpus do link {link}:\n  {corpus}')